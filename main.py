import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
from sentence_transformers import SentenceTransformer
import requests
import webbrowser
import threading
import time
import pytesseract
from PIL import Image
import io
import torch
import cv2
import concurrent.futures
from tqdm import tqdm

# GPU ì´ˆê¸°í™” ë° í™•ì¸
def init_gpu():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        torch.backends.cudnn.benchmark = True
        print(f"GPU initialized: {torch.cuda.get_device_name(0)}")
        return True
    else:
        print("GPU not available, using CPU instead")
        return False

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (GPU ê°€ì†)
def preprocess_image(img):
    # OpenCVë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬ (GPU ê°€ì†)
    if isinstance(img, bytes) or isinstance(img, io.BytesIO):
        if isinstance(img, bytes):
            img = io.BytesIO(img)
        img = np.array(Image.open(img))
    
    # BGRë¡œ ë³€í™˜ (OpenCV í˜•ì‹)
    if img.shape[2] == 4:  # RGBA ì´ë¯¸ì§€ì¸ ê²½ìš°
        img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)
    elif img.shape[2] == 3:  # RGB ì´ë¯¸ì§€ì¸ ê²½ìš°
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    
    # ì´ë¯¸ì§€ ê°œì„  (GPU ê°€ì† í™œìš©)
    if cv2.cuda.getCudaEnabledDeviceCount() > 0:  # CUDA ì§€ì› í™•ì¸
        gpu_img = cv2.cuda_GpuMat()
        gpu_img.upload(img)
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ (GPU)
        gpu_gray = cv2.cuda.cvtColor(gpu_img, cv2.COLOR_BGR2GRAY)
        
        # ë…¸ì´ì¦ˆ ì œê±° (GPU)
        gpu_denoised = cv2.cuda.fastNlMeansDenoising(gpu_gray)
        
        # ëŒ€ë¹„ í–¥ìƒ (GPU)
        gpu_clahe = cv2.cuda.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        gpu_enhanced = gpu_clahe.apply(gpu_denoised)
        
        # CPUë¡œ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
        enhanced_img = gpu_enhanced.download()
    else:
        # CPU ëŒ€ì²´ ì²˜ë¦¬
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        denoised = cv2.fastNlMeansDenoising(gray)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced_img = clahe.apply(denoised)
    
    return enhanced_img

# ë³‘ë ¬ OCR ì²˜ë¦¬ í•¨ìˆ˜
def process_page_with_ocr(page, use_ocr=True):
    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
    text = page.get_text()
    
    # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ê³  OCRì´ í™œì„±í™”ëœ ê²½ìš°, ì´ë¯¸ì§€ ê¸°ë°˜ OCR ìˆ˜í–‰
    if use_ocr and len(text.strip()) < 100:  # í…ìŠ¤íŠ¸ê°€ ì ë‹¤ë©´ ì´ë¯¸ì§€ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
        # í˜ì´ì§€ë¥¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§
        pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))  # í•´ìƒë„ 3ë°° ì¦ê°€
        img_bytes = pix.tobytes("png")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        enhanced_img = preprocess_image(img_bytes)
        
        # ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜í•˜ì—¬ OCR ì²˜ë¦¬
        ocr_text = pytesseract.image_to_string(
            Image.fromarray(enhanced_img), 
            lang='kor+eng',  # í•œêµ­ì–´+ì˜ì–´ OCR
            config='--oem 1 --psm 6 -c tessedit_do_invert=0'  # LSTM OCR ì—”ì§„, ê· ì¼í•œ í…ìŠ¤íŠ¸ ë¸”ë¡
        )
        
        # OCR ê²°ê³¼ê°€ ë” ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë©´ ì‚¬ìš©
        if len(ocr_text.strip()) > len(text.strip()):
            text = ocr_text
    
    return text

# OCR ê¸°ëŠ¥ì„ í†µí•©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (GPU ê°€ì† ë° ë³‘ë ¬ ì²˜ë¦¬)
def extract_text_from_pdf(pdf_path, use_ocr=True):
    # GPU ì´ˆê¸°í™”
    has_gpu = init_gpu()
    
    doc = fitz.open(pdf_path)
    total_pages = len(doc)
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì„¤ì •
    # PDF ì²˜ë¦¬ëŠ” I/O ë°”ìš´ë“œì™€ CPU ë°”ìš´ë“œ ì‘ì—…ì´ ì„ì—¬ ìˆìœ¼ë¯€ë¡œ ThreadPoolExecutorê°€ ì í•©
    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        # ê° í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        future_to_page = {executor.submit(process_page_with_ocr, doc[page_num], use_ocr): page_num for page_num in range(total_pages)}
        
        # ê²°ê³¼ ìˆ˜ì§‘ (í˜ì´ì§€ ìˆœì„œ ìœ ì§€)
        text_content = [""] * total_pages
        
        # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
        for future in tqdm(concurrent.futures.as_completed(future_to_page), total=total_pages, desc="OCR ì²˜ë¦¬ ì¤‘"):
            page_num = future_to_page[future]
            try:
                text_content[page_num] = future.result()
            except Exception as e:
                print(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                text_content[page_num] = ""
    
    return "\n".join(text_content)

# 2. í…ìŠ¤íŠ¸ â†’ ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°
def chunk_text(text, size=500):
    return [text[i:i+size] for i in range(0, len(text), size)]

# 3. ë¬¸ë‹¨ â†’ ì„ë² ë”© + ì¸ë±ì‹± (GPU ê°€ì†)
def build_faiss(chunks, embed_model):
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    use_gpu = torch.cuda.is_available()
    
    # ì„ë² ë”© ìƒì„± (GPU ì‚¬ìš©)
    # SentenceTransformerëŠ” ìë™ìœ¼ë¡œ GPUë¥¼ ê°ì§€í•˜ê³  ì‚¬ìš©í•¨
    embeddings = embed_model.encode(chunks, batch_size=32, show_progress_bar=True)
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    dimension = embeddings.shape[1]
    
    if use_gpu:
        # GPU ê°€ì† ì¸ë±ìŠ¤ ìƒì„±
        # ë¨¼ì € CPUì—ì„œ ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.IndexFlatL2(dimension)
        
        # GPUë¡œ ì¸ë±ìŠ¤ ì´ë™
        res = faiss.StandardGpuResources()
        gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
        
        # ì„ë² ë”© ì¶”ê°€
        gpu_index.add(np.array(embeddings).astype(np.float32))
        
        # ê²€ìƒ‰ì„ ìœ„í•´ CPUë¡œ ë‹¤ì‹œ ì´ë™ (ì„ íƒì‚¬í•­)
        index = faiss.index_gpu_to_cpu(gpu_index)
    else:
        # CPU ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.IndexFlatL2(dimension)
        index.add(np.array(embeddings).astype(np.float32))
    
    return index, embeddings, chunks

# 4. ì§ˆì˜ â†’ ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰ (GPU ê°€ì†)
def search_similar(query, embed_model, index, chunks, k=3):
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (ì„ë² ë”© ê³„ì‚°ì— ì‚¬ìš©)
    use_gpu = torch.cuda.is_available()
    
    # ì¿¼ë¦¬ ì„ë² ë”© ê³„ì‚° (GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ ìë™ í™œìš©)
    q_emb = embed_model.encode([query])
    
    # GPUë¡œ ê²€ìƒ‰
    if use_gpu and 'StandardGpuResources' in dir(faiss):
        # GPUë¡œ ì¸ë±ìŠ¤ ì´ë™
        res = faiss.StandardGpuResources()
        gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
        
        # GPUë¡œ ê²€ìƒ‰
        D, I = gpu_index.search(np.array(q_emb).astype(np.float32), k)
    else:
        # CPUë¡œ ê²€ìƒ‰
        D, I = index.search(np.array(q_emb).astype(np.float32), k)
    
    # ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²°ê³¼ ë°˜í™˜
    results = []
    for i, dist in zip(I[0], D[0]):
        if i < len(chunks):
            results.append({
                'chunk': chunks[i],
                'similarity_score': 1.0 / (1.0 + dist)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜
            })
    
    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ë‹¨ë§Œ ë°˜í™˜
    return [item['chunk'] for item in results]

# 5. ë¡œì»¬ LLMì— í”„ë¡¬í”„íŠ¸ ìš”ì²­
def query_local_llm(prompt, model_name, api_url="http://127.0.0.1:1234/v1/chat/completions"):
    headers = {"Content-Type": "application/json"}
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì„¤ëª…í•´ì£¼ëŠ” ì¡°ìˆ˜ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }
    try:
        res = requests.post(api_url, json=payload, headers=headers)
        return res.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Error: {str(e)}"

# PDF íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_pdf_files():
    directory = "./docs"
    if not os.path.exists(directory):
        os.makedirs(directory)
    return [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]

# ê·¸ë¼ë””ì˜¤ ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
def analyze_pdf(pdf_filename, user_question, model_name, api_url, use_ocr, use_gpu):
    if not pdf_filename:
        return "PDF íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
    
    if not user_question:
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    if not model_name:
        return "LLM ëª¨ë¸ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    if not api_url:
        api_url = "http://127.0.0.1:1234/v1/chat/completions"
    
    try:
        # GPU ê°€ìš©ì„± í™•ì¸
        gpu_available = torch.cuda.is_available()
        if use_gpu and not gpu_available:
            return "GPU ì‚¬ìš©ì„ ì„ íƒí–ˆì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥í•œ GPUê°€ ì—†ìŠµë‹ˆë‹¤. GPU ë“œë¼ì´ë²„ ì„¤ì¹˜ ë° CUDA í˜¸í™˜ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # ì „ì²´ ê²½ë¡œ êµ¬ì„±
        pdf_path = os.path.join("./docs", pdf_filename)
        
        # ì‹¤í–‰ ì •ë³´ ë° ì§„í–‰ ìƒí™©
        progress_text = f"ğŸ“„ '{pdf_filename}' íŒŒì¼ ì²˜ë¦¬ ì¤‘...\n"
        
        # GPU ì •ë³´ í‘œì‹œ
        if use_gpu and gpu_available:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB ë‹¨ìœ„ë¡œ ë³€í™˜
            progress_text += f"ğŸš€ GPU ê°€ì† í™œì„±í™”: {gpu_name} ({gpu_memory:.2f} GB)\n"
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (GPU ìë™ ì‚¬ìš©)
        progress_text += "ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\n"
        embed_model = SentenceTransformer("all-MiniLM-L6-v2")
        
        # GPU/CUDA í™•ì¸
        if use_gpu and gpu_available:
            embed_model = embed_model.to(torch.device("cuda"))
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼)
        if use_ocr:
            progress_text += "ğŸ” OCRì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ (ë³‘ë ¬ ì²˜ë¦¬)...\n"
        else:
            progress_text += "ğŸ“‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\n"
        
        # ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        raw_text = extract_text_from_pdf(pdf_path, use_ocr=use_ocr)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œê°„ ê³„ì‚°
        extraction_time = time.time() - start_time
        progress_text += f"â±ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({extraction_time:.2f}ì´ˆ)\n"
        
        # ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°
        chunks = chunk_text(raw_text)
        progress_text += f"ğŸ“‹ ì´ {len(chunks)}ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• \n"
        
        # ì„ë² ë”© ë° ì¸ë±ì‹± ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        # ì„ë² ë”© ë° ì¸ë±ì‹±
        progress_text += "ğŸ“ ì„ë² ë”© ë° ì¸ë±ì‹± ì¤‘ (GPU ê°€ì†)...\n"
        index, embeddings, chunks = build_faiss(chunks, embed_model)
        
        # ì„ë² ë”© ì‹œê°„ ê³„ì‚°
        embedding_time = time.time() - start_time
        progress_text += f"â±ï¸ ì„ë² ë”© ì™„ë£Œ ({embedding_time:.2f}ì´ˆ)\n"
        
        # ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰
        progress_text += "ğŸ” ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ë‹¨ ê²€ìƒ‰ ì¤‘...\n"
        related = search_similar(user_question, embed_model, index, chunks)
        
        # ë¡œì»¬ AIì— ì§ˆì˜
        progress_text += f"ğŸ§  ë¡œì»¬ AI ({model_name})ì—ê²Œ ì§ˆì˜ ì¤‘...\n\n"
        context = "\n\n".join(related)
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
---
{context}
---

ì§ˆë¬¸: {user_question}

ë‹µë³€ì„ í•  ë•ŒëŠ” ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""
        
        # LLM ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        answer = query_local_llm(prompt, model_name, api_url)
        llm_time = time.time() - start_time
        
        # ì´ ì†Œìš” ì‹œê°„ ì •ë³´ ì¶”ê°€
        progress_text += f"â±ï¸ AI ì‘ë‹µ ìƒì„± ì™„ë£Œ ({llm_time:.2f}ì´ˆ)\n"
        
        # ê²°ê³¼ ë°˜í™˜
        result = f"{progress_text}\n-------- ë¶„ì„ ê²°ê³¼ --------\n\n{answer}"
        return result
    
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜: {error_details}"

# ë¸Œë¼ìš°ì € ìë™ ì‹¤í–‰ í•¨ìˆ˜
def open_browser(port=7860):
    def _open_browser():
        # ì„œë²„ê°€ ì‹œì‘ë  ì‹œê°„ì„ ì£¼ê¸° ìœ„í•´ 2ì´ˆ ëŒ€ê¸°
        time.sleep(2)
        # ê¸°ë³¸ ë¸Œë¼ìš°ì €ë¡œ Gradio í˜ì´ì§€ ì—´ê¸°
        webbrowser.open(f'http://127.0.0.1:{port}')
    
    # ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ë¸Œë¼ìš°ì € ì—´ê¸°
    threading.Thread(target=_open_browser).start()

# ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ í•¨ìˆ˜
def get_system_info():
    info = {
        "cuda_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        "gpu_names": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else [],
        "cpu_count": os.cpu_count(),
    }
    
    # OpenCV CUDA ì§€ì› í™•ì¸
    if hasattr(cv2, 'cuda') and hasattr(cv2.cuda, 'getCudaEnabledDeviceCount'):
        info["opencv_cuda"] = cv2.cuda.getCudaEnabledDeviceCount() > 0
    else:
        info["opencv_cuda"] = False
    
    # FAISS GPU ì§€ì› í™•ì¸
    info["faiss_gpu"] = hasattr(faiss, 'StandardGpuResources')
    
    return info

# GPU ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
def check_gpu_availability():
    system_info = get_system_info()
    if system_info["cuda_available"]:
        gpu_names = ", ".join(system_info["gpu_names"])
        return f"âœ… GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥: {gpu_names}"
    else:
        return "âŒ GPU ê°€ì† ë¶ˆê°€: CUDA ì§€ì› GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."


# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
with gr.Blocks(title="PDF ë¶„ì„ê¸° (GPU ê°€ì†)") as app:
    gr.Markdown("# ğŸš€ PDF ë¶„ì„ê¸° (OCR + GPU ê°€ì†)")
    
    # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
    system_info = get_system_info()
    if system_info["cuda_available"]:
        gpu_info = f"ğŸ’» ì‹œìŠ¤í…œ: CPU {system_info['cpu_count']}ì½”ì–´, GPU {system_info['gpu_count']}ëŒ€ ({', '.join(system_info['gpu_names'])})"
        gr.Markdown(f"{gpu_info}\n\nìŠ¤ìº”ëœ ë¬¸ì„œëŠ” OCRì„ í™œì„±í™”í•˜ê³ , ì²˜ë¦¬ ì†ë„ í–¥ìƒì„ ìœ„í•´ GPU ê°€ì†ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    else:
        gr.Markdown("ğŸ’» GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n\nìŠ¤ìº”ëœ ë¬¸ì„œëŠ” OCRì„ í™œì„±í™”í•˜ì„¸ìš”.")
    
    with gr.Row():
        with gr.Column(scale=1):
            # PDF íŒŒì¼ ì„ íƒ ë“œë¡­ë‹¤ìš´
            pdf_dropdown = gr.Dropdown(
                label="PDF íŒŒì¼ ì„ íƒ", 
                choices=get_pdf_files(),
                interactive=True
            )
            
            # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
            refresh_btn = gr.Button("ğŸ“‚ íŒŒì¼ ëª©ë¡ ìƒˆë¡œê³ ì¹¨")
            
            with gr.Row():
                # OCR ì‚¬ìš© ì—¬ë¶€ ì²´í¬ë°•ìŠ¤
                ocr_checkbox = gr.Checkbox(
                    label="OCR í™œì„±í™”",
                    value=True,
                    info="ìŠ¤ìº”ëœ ë¬¸ì„œë‚˜ ì´ë¯¸ì§€ PDFì— í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ë•Œ OCRì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )
                
                # GPU ì‚¬ìš© ì—¬ë¶€ ì²´í¬ë°•ìŠ¤
                gpu_checkbox = gr.Checkbox(
                    label="GPU ê°€ì†",
                    value=system_info["cuda_available"],  # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ê¸°ë³¸ í™œì„±í™”
                    interactive=system_info["cuda_available"],  # GPU ì—†ìœ¼ë©´ ë¹„í™œì„±í™”
                    info="CUDA GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
                )
            
            # ì§ˆë¬¸ ì…ë ¥
            question_input = gr.Textbox(
                label="ì§ˆë¬¸ ì…ë ¥",
                placeholder="ì˜ˆ: ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜",
                lines=3
            )
            
            # LLM ëª¨ë¸ ì´ë¦„ ì…ë ¥ (í…ìŠ¤íŠ¸ í•„ë“œ)
            model_input = gr.Textbox(
                label="LM Studio ëª¨ë¸ ì´ë¦„ ì…ë ¥",
                placeholder="ëª¨ë¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: llama-dna-1.0-8b-instruct)",
                value="llama-dna-1.0-8b-instruct"
            )
            
            # API URL ì…ë ¥ - ì´ ë¶€ë¶„ì´ ì˜ë ¸ì—ˆìŠµë‹ˆë‹¤
            api_url_input = gr.Textbox(
                label="API URL (ì„ íƒì‚¬í•­)",
                placeholder="http://127.0.0.1:1234/v1/chat/completions",
                value="http://127.0.0.1:1234/v1/chat/completions"
            )
            
            # ë¶„ì„ ë²„íŠ¼
            analyze_btn = gr.Button("ğŸ” ë¶„ì„í•˜ê¸°", variant="primary")
        
        with gr.Column(scale=2):
            # ê²°ê³¼ ì¶œë ¥
            result_output = gr.Textbox(
                label="ë¶„ì„ ê²°ê³¼",
                lines=15,
                placeholder="ë¶„ì„ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤..."
            )
    
    # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ ì´ë²¤íŠ¸
    def refresh_pdf_list():
        return gr.Dropdown(choices=get_pdf_files())
    
    refresh_btn.click(refresh_pdf_list, outputs=[pdf_dropdown])
    
    # ë¶„ì„ ë²„íŠ¼ ì´ë²¤íŠ¸
    analyze_btn.click(
        analyze_pdf, 
        inputs=[pdf_dropdown, question_input, model_input, api_url_input, ocr_checkbox, gpu_checkbox], 
        outputs=[result_output]
    )
    
    # ë„ì›€ë§ í‘œì‹œ
    gr.Markdown("""
    ## ì‚¬ìš© ë°©ë²•
    1. docs í´ë”ì— ë¶„ì„í•  PDF íŒŒì¼ì„ ë„£ìœ¼ì„¸ìš”.
    2. PDF íŒŒì¼ì„ ì„ íƒí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.
    3. ìŠ¤ìº”ëœ ë¬¸ì„œë‚˜ ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¸ ê²½ìš° "OCR í™œì„±í™”" ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.
    4. GPUê°€ ìˆëŠ” ê²½ìš° "GPU ê°€ì†" ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ë©´ ì²˜ë¦¬ ì†ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.
    5. LM Studioì—ì„œ ë¡œë“œí•œ ëª¨ë¸ì˜ ì´ë¦„ì„ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”.
    6. API URLì´ ê¸°ë³¸ê°’ê³¼ ë‹¤ë¥¸ ê²½ìš° ìˆ˜ì •í•˜ì„¸ìš”.
    7. 'ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.
    
    ## ì£¼ì˜ì‚¬í•­
    - OCR ì²˜ë¦¬ëŠ” ì‹œê°„ì´ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì§€ë§Œ, GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë©´ ì†ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.
    - í•œêµ­ì–´ì™€ ì˜ì–´ OCRì„ ì§€ì›í•©ë‹ˆë‹¤.
    - GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ CUDAì™€ í˜¸í™˜ë˜ëŠ” NVIDIA GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.
    - LM Studioì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ ì´ë¦„ì„ ì •í™•íˆ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    - LM Studio API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    - OCR ì‚¬ìš©ì„ ìœ„í•´ pytesseractì™€ Tesseract OCR ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    """)

# ê·¸ë¼ë””ì˜¤ ì•± ì‹¤í–‰
if __name__ == "__main__":
    # ë¸Œë¼ìš°ì € ìë™ ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ
    open_browser()
    
    # Gradio ì•± ì‹¤í–‰
    app.launch(share=False)
